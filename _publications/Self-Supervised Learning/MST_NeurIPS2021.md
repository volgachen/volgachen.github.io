---
title:          "MST: Masked Self-Supervised Transformer for Visual Representation"
date:           2021-10-31 00:01:00 +0800
selected:       true
pub:            "Neural Information Processing Systems (NeurIPS)"
pub_date:       "2021"
category:       "3. Self-Supervised Learning"
abstract: >-
  This paper is an early work to introduce Masked Image Modeling in self-supervised learning. MST utilizes self-attention map to mask background image tokens, and supervises with a pixel-level restoration loss to preserve fine-grained information, in addition to common contrastive learning. MST helps a lot in downstream tasks.
cover:          /assets/images/covers/MST_NeurIPS2021.png
authors:
- Zhaowen Li
- Zhiyang Chen
- Fan Yang
- Wei Li
- Yousong Zhu
- Chaoyang Zhao
- Rui Deng
- Liwei Wu
- Rui Zhao
- Ming Tang
- Jinqiao Wang
links:
  Paper: https://arxiv.org/abs/2106.05656
---

